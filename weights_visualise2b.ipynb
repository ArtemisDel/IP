{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s1955786/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/s1955786/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/s1955786/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/s1955786/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/s1955786/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/s1955786/miniconda3/envs/py37/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_1d, global_max_pool\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.estimator import regression\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['KERAS_BACKEND']='theano'\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, merge, Dropout, LSTM, GRU, Bidirectional\n",
    "from keras.models import Model,Sequential\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers, optimizers\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import re\n",
    "import pdb\n",
    "import pickle\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import theano\n",
    "import keras\n",
    "#for visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML as html_print\n",
    "from IPython.display import display\n",
    "\n",
    "#define custom attention layer\n",
    "class AttLayer(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                      shape=(input_shape[-1],),\n",
    "                                      initializer='random_normal',\n",
    "                                      trainable=True)\n",
    "        super(AttLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "        \n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "        \n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "\n",
    "filename = \"checkpoint.model.keras\"\n",
    "model = load_model(filename, custom_objects={'AttLayer': AttLayer})\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "#data \n",
    "\n",
    "print(f\"Imports loaded successfully\")\n",
    "\n",
    "#define data\n",
    "eval_data=\"eval_formspring\"\n",
    "data=\"formspring\"\n",
    "MAX_FEATURES = 2\n",
    "\n",
    "def get_filename(dataset):\n",
    "    global NUM_CLASSES\n",
    "    if(dataset==\"twitter\"):\n",
    "        NUM_CLASSES = 3 #! CHANGE HERE TO 2 from 3\n",
    "        # HASH_REMOVE = True\n",
    "        filename = \"data/twitter_data.pkl\"\n",
    "    elif(dataset==\"formspring\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/formspring_data.pkl\"\n",
    "    elif(dataset==\"wiki\"):\n",
    "        NUM_CLASSES = 2\n",
    "        filename = \"data/wiki_data.pkl\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def load_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    x_text = []\n",
    "    labels = [] \n",
    "    for i in range(len(data)):\n",
    "        # if(HASH_REMOVE):\n",
    "        #     x_text.append(p.tokenize((data[i]['text']).encode('utf-8')))\n",
    "        # else:\n",
    "        # #     x_text.append(data[i]['body']) #change here from 'text' to 'body'\n",
    "        # labels.append(data[i]['level_1']) #changed here from 'label' to 'level_1'\n",
    "        x_text.append(data.loc[i]['body'])\n",
    "        labels.append(data.loc[i]['level_1'])\n",
    "        \n",
    "\n",
    "    return x_text,labels\n",
    "\n",
    "def get_eval_filename(dataset):\n",
    "    global NUM_CLASSES\n",
    "    if(dataset==\"eval_formspring\"):\n",
    "        NUM_CLASSES = 2\n",
    "        # pdb.set_trace()\n",
    "        eval_filename = \"data/eval_formspring_data.pkl\"\n",
    "        # pdb.set_trace()\n",
    "    elif(dataset==\"wiki\"):\n",
    "        NUM_CLASSES = 2\n",
    "        eval_filename = \"data/wiki_data.pkl\"\n",
    "    return eval_filename\n",
    "\n",
    "def load_eval_data(filename):\n",
    "    print(\"Loading data from file: \" + filename)\n",
    "    data = pickle.load(open(filename, 'rb'))\n",
    "    val_text = []\n",
    "    val_labels = [] \n",
    "    for i in range(len(data)):\n",
    "        #     x_text.append(data[i]['body']) #change here from 'text' to 'body'\n",
    "        # labels.append(data[i]['level_1']) #changed here from 'label' to 'level_1'\n",
    "        val_text.append(data.loc[i]['body'])\n",
    "        val_labels.append(data.loc[i]['level_1'])\n",
    "    return val_text, val_labels  \n",
    "\n",
    "#process data\n",
    "def get_train_test(data, eval_data, x_text, eval_text, labels, eval_labels):\n",
    "    \n",
    "    X_train=x_text\n",
    "    Y_train=labels\n",
    "\n",
    "    X_test=eval_text\n",
    "    Y_test=eval_labels\n",
    "\n",
    "    # for train data\n",
    "    post_length = np.array([len(x.split(\" \")) for x in x_text])\n",
    "    if(data != \"twitter\"):\n",
    "        max_document_length = int(np.percentile(post_length, 95))\n",
    "    else:\n",
    "        max_document_length = max(post_length)\n",
    "    print(\"Document length : \" + str(max_document_length))\n",
    "\n",
    "    #for evaluation data\n",
    "    eval_post_length = np.array([len(x.split(\" \")) for x in eval_text])\n",
    "    if(eval_data != \"twitter\"):\n",
    "        eval_max_document_length = int(np.percentile(eval_post_length, 95))\n",
    "    else:\n",
    "        eval_max_document_length = max(eval_post_length)\n",
    "    print(\"Document length : \" + str(eval_max_document_length))\n",
    "    \n",
    "    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length, MAX_FEATURES)\n",
    "    vocab_processor = vocab_processor.fit(x_text)\n",
    "\n",
    "    trainX = np.array(list(vocab_processor.transform(X_train)))\n",
    "    testX = np.array(list(vocab_processor.transform(X_test)))\n",
    "    \n",
    "    # Perform label encoding\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_mapping={\"Misogynistic\":1, \"Nonmisogynistic\":0} #label mapping\n",
    "    label_mapping = {label: int(encoded_label) for label, encoded_label in label_mapping.items()}\n",
    "    for label, encoded_label in label_mapping.items():\n",
    "        print(f\"Type of {label}: {type(encoded_label)}\")\n",
    "    Y_train_encoded = [label_mapping[label] for label in Y_train]\n",
    "    \n",
    "    Y_test_encoded = [label_mapping[label] for label in Y_test] \n",
    "  \n",
    "    Y_test_encoded_final=Y_test_encoded\n",
    "    Y_train_encoded_final = Y_train_encoded\n",
    "    print(type(Y_test_encoded_final))\n",
    "    print(type(Y_train_encoded_final))\n",
    "    print(\"Original Y_train labels:\", Y_train)\n",
    "    print(\"Encoded Y_train labels:\", Y_train_encoded_final)\n",
    "\n",
    "    print(\"Original Y_test labels:\", Y_test)\n",
    "    print(\"Encoded Y_test labels:\", Y_test_encoded_final)\n",
    "    for label, encoded_label in label_mapping.items():\n",
    "        print(f\"Label: {label}, Encoded value: {encoded_label}\")\n",
    "\n",
    "\n",
    "    trainY = to_categorical(Y_train_encoded_final, nb_classes=NUM_CLASSES)\n",
    "    testY = to_categorical(Y_test_encoded_final, nb_classes=NUM_CLASSES)\n",
    "\n",
    "    testY = testY.astype(np.float64)\n",
    "    trainY = trainY.astype(np.float64)\n",
    "    print(f'testy.dtype {testY.dtype}')   # Check data type of testY\n",
    "    print(f'trainy.dtype {trainY.dtype}')   # Check data type of trainY\n",
    "        \n",
    "    trainX = pad_sequences(trainX, maxlen=max_document_length, value=0.)\n",
    "    testX = pad_sequences(testX, maxlen=eval_max_document_length, value=0.)\n",
    "    \n",
    "    data_dict = {\n",
    "        \"data\": data,\n",
    "        \"trainX\" : trainX,\n",
    "        \"trainY\" : trainY,\n",
    "        \"testX\" : testX,\n",
    "        \"testY\" : testY,\n",
    "        \"vocab_processor\" : vocab_processor\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "def return_data(data_dict):\n",
    "    return data_dict[\"data\"],data_dict[\"trainX\"], data_dict[\"trainY\"], data_dict[\"testX\"], data_dict[\"testY\"], data_dict[\"vocab_processor\"]\n",
    "\n",
    "\n",
    "x_text, labels = load_data(get_filename(data)) \n",
    "# pdb.set_trace()\n",
    "eval_text, eval_labels = load_eval_data(get_eval_filename(eval_data)) \n",
    "# pdb.set_trace()\n",
    "data_dict=get_train_test(data, eval_data, x_text, eval_text, labels, eval_labels)\n",
    "# pdb.set_trace()\n",
    "\n",
    "data, trainX, trainY, testX, testY, vocab_processor = return_data(data_dict)\n",
    "\n",
    "#dynamically pad sequences\n",
    "max_sequence_length = 303 #from the model\n",
    "testX_padded = pad_sequences(testX, maxlen=max_sequence_length, padding='post')\n",
    "trainX_padded= pad_sequences(trainX, maxlen=max_sequence_length, padding='post')\n",
    "print(testX_padded.shape)\n",
    "print(testY.shape)\n",
    "print(trainX_padded.shape)\n",
    "print(trainY.shape)\n",
    "print(\"Input Data (testX_padded):\")\n",
    "print(testX_padded[:5])  # Print the first 5 samples\n",
    "\n",
    "print(\"Labels (testY):\")\n",
    "print(testY[:5])\n",
    "\n",
    "\n",
    "attention_model = Model(inputs=model.input,\n",
    "                        outputs=[model.output, model.get_layer('att_layer_1').output])\n",
    "\n",
    "\n",
    "predictions, attention_weights = attention_model.predict(testX_padded)\n",
    "\n",
    "# Plot the attention weights as a heatmap\n",
    "# pdb.set_trace()\n",
    "# sns.heatmap(np.expand_dims(attention_weights[0], axis=0), annot=True, cmap=\"YlGnBu\", xticklabels=x_text[0].split(), yticklabels=False)\n",
    "# plt.xlabel('Input Text')\n",
    "# plt.ylabel('Attention Weights')\n",
    "# plt.title('Attention Visualization')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "def get_clr(value):\n",
    "    # pdb.set_trace()\n",
    "    colors = ['#85c2e1', '#89c4e2', '#95cae5', '#99cce6', '#a1d0e8'\n",
    "        '#b2d9ec', '#baddee', '#c2e1f0', '#eff7fb', '#f9e8e8',\n",
    "        '#f9e8e8', '#f9d4d4', '#f9bdbd', '#f8a8a8', '#f68f8f',\n",
    "        '#f47676', '#f45f5f', '#f34343', '#f33b3b', '#f42e2e']\n",
    "    normalized_value = (value - (-10)) / (10 - (-10))\n",
    "    scaled_value = int(normalized_value * 19)\n",
    "    # value = int((value * 2))\n",
    "    # pdb.set_trace()\n",
    "    return colors[scaled_value]\n",
    "\n",
    "def visualize(output_values, result_list, cell_no):\n",
    "    print(\"\\nCell Number:\", cell_no, \"\\n\")\n",
    "    text_colours = []\n",
    "    for i in range(len(output_values)):\n",
    "        # pdb.set_trace()\n",
    "        text = (result_list[i], get_clr(output_values[i][cell_no]))\n",
    "        # pdb.set_trace()\n",
    "        text_colours.append(text)\n",
    "    print_color(text_colours)\n",
    "\n",
    "def print_color(t):\n",
    "    display(html_print(''.join([cstr(ti, color=ci) for ti,ci in t])))\n",
    "\n",
    "# def cstr(s, color='black'):\n",
    "#     if s == ' ':\n",
    "#         return \"\".format(color, s)\n",
    "#     else:\n",
    "#         return \"\".format(color, s)\n",
    "\n",
    "def cstr(s, color='black'):\n",
    "    if isinstance(s, str) and s == ' ':\n",
    "        return \"\".format(color, s)\n",
    "    else:\n",
    "        return \"\".format(color, s)\n",
    "\n",
    "for cell_no in [0, 1]:\n",
    "    visualize(predictions, attention_weights, cell_no)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
